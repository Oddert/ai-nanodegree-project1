{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf016aa2-7cdd-47dd-94c6-3b288cbf16f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-11 12:37:26.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mpackages imported\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Credits\n",
    "--------\n",
    "\n",
    "\"Fine-Tuning BERT for text classification with LoRA\" by Karkar Nizar - Used to verify and compare PEFT configuration when previous datasets were causing issues (Available at: https://medium.com/@karkar.nizar/fine-tuning-bert-for-text-classification-with-lora-f12af7fa95e4)\n",
    "compute_metrics() - taken from an earlier instructional lesson\n",
    "Dataset \"Ukrainian Formality Dataset (translated)\" available at: https://huggingface.co/datasets/ukr-detect/ukr-formality-dataset-translated-gyafc\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from loguru import logger\n",
    "from peft import (\n",
    "\tAutoPeftModelForSequenceClassification,\n",
    "\tget_peft_model,\n",
    "\tLoraConfig,\n",
    "\tTaskType,\n",
    ")\n",
    "from transformers import (\n",
    "\tAutoTokenizer,\n",
    "\t# DistilBertForSequenceClassification,\n",
    "\tBertForSequenceClassification,\n",
    "\tTrainer,\n",
    "\tTrainingArguments,\n",
    ")\n",
    "logger.info('packages imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e862b1-a8fb-4b4e-b7dc-bb22ab698c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-09 11:43:58.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1msettings created\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'bert-base-cased'\n",
    "# model_name = 'distilbert-base-uncased'\n",
    "\n",
    "dataset_name = 'ukr-detect/ukr-formality-dataset-translated-gyafc'\n",
    "# dataset_name = 'dair-ai/emotion'\n",
    "# dataset_name = 'roman_urdu_hate_speech'\n",
    "# dataset_name = 'ctoraman/gender-hate-speech'\n",
    "# dataset_name = 'imdb'\n",
    "\n",
    "tokeniser_key = 'text'\n",
    "train_key = 'train'\n",
    "test_key = 'test'\n",
    "initial_save_name = './data/initial'\n",
    "final_save_name = './data/final'\n",
    "select_size = 1000\n",
    "num_train_epochs = 10\n",
    "\n",
    "logger.info('settings created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8027b3-8ec4-43d0-ad6f-b82239981088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209124/209124 [01:49<00:00, 1906.08 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10272/10272 [00:03<00:00, 3098.55 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4853/4853 [00:01<00:00, 3240.62 examples/s]\n",
      "\u001b[32m2024-06-09 11:46:40.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mtokenised dataset loaded\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenise(examples):\n",
    "\treturn tokeniser(\n",
    "        examples[tokeniser_key],\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "tokenised_ds = dataset.map(tokenise, batched=True)\n",
    "\n",
    "sampled_ds = {}\n",
    "\n",
    "for label in [train_key, test_key]:\n",
    "      sampled_ds[label] = tokenised_ds[label].shuffle(seed=202405241534).select(range(select_size))\n",
    "      sampled_ds[label] = sampled_ds[label].rename_column('labels', 'label')\n",
    "\n",
    "# train_ds = train_ds.rename_column('Text', 'text')\n",
    "# train_ds = train_ds.rename_column('Label', 'label')\n",
    "\n",
    "# test_ds = test_ds.rename_column('Text', 'text')\n",
    "# test_ds = test_ds.rename_column('Label', 'label')\n",
    "logger.info('tokenised dataset loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53cdfc55-3e53-4e39-907e-c882f21034db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-06-09 11:46:50.548\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[34m\u001b[1mBertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-06-09 11:46:50.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mbase model instantiated\u001b[0m\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-06-09 11:46:50.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1minitial trainer instantiated\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Loading and Evaluating a Foundation Model\n",
    "## Loading the model\n",
    "# Once you have selected a model, load it in your notebook.\n",
    "base_model = BertForSequenceClassification.from_pretrained(\n",
    "\tmodel_name,\n",
    "\tnum_labels=2,\n",
    "\tid2label={\n",
    "\t\t0: 'INFORMAL',\n",
    "\t\t1: 'FORMAL',\n",
    "\t},\n",
    "\tlabel2id={\n",
    "\t\t'INFORMAL': 0,\n",
    "\t\t'FORMAL': 1,\n",
    "\t}\n",
    ")\n",
    "logger.debug(base_model)\n",
    "logger.info('base model instantiated')\n",
    "# base_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "# \tmodel_name,\n",
    "# \tnum_labels=2,\n",
    "# )\n",
    "\n",
    "# TODO: Copied from earlier lesson, potentially upgrade to something original\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': (predictions == labels).mean()}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\tevaluation_strategy='epoch',\n",
    "\tload_best_model_at_end=True,\n",
    "\tnum_train_epochs=num_train_epochs,\n",
    "\toutput_dir='./data/project1/initial',\n",
    "\tsave_strategy='epoch',\n",
    ")\n",
    "\n",
    "# NOTE: I was unsure if the intent was to instantiate a PEFT version of the base model and evaluate that, but instead instantiated a Trainer in order to evaluate the base without training.\n",
    "trainer = Trainer(\n",
    "\targs=training_args,\n",
    "\tcompute_metrics=compute_metrics,\n",
    "\tmodel=base_model,\n",
    "\teval_dataset=sampled_ds[test_key],\n",
    "\ttrain_dataset=sampled_ds[train_key],\n",
    ")\n",
    "logger.info('initial trainer instantiated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5e71e4-c255-4317-8ed6-3f6eee9f272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 2:48:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-09 14:36:14.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1minitial trainer evaluation complete\u001b[0m\n",
      "\u001b[32m2024-06-09 14:36:14.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mThe base model was evaluated with the following metrics:\u001b[0m\n",
      "\u001b[32m2024-06-09 14:36:14.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1m{'eval_loss': 0.6910282373428345, 'eval_accuracy': 0.55, 'eval_runtime': 10143.1314, 'eval_samples_per_second': 0.099, 'eval_steps_per_second': 0.012}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Evaluating the model\n",
    "# Perform an initial evaluation of the model on your chosen sequence classification task. This step will require that you also load an appropriate tokenizer and dataset.\n",
    "initial_evaluation = trainer.evaluate()\n",
    "logger.info('initial trainer evaluation complete')\n",
    "logger.info('The base model was evaluated with the following metrics:')\n",
    "logger.info(initial_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4af0ad7-0a2d-4c1d-bd75-8cb4cf4a33c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-09 22:43:31.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mpeft config created\u001b[0m\n",
      "\u001b[32m2024-06-09 22:43:31.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mlora model instantiated\u001b[0m\n",
      "\u001b[32m2024-06-09 22:43:31.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mpeft trainer instantiated\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 38,402 || all params: 108,350,212 || trainable%: 0.0354\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 37:04:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.685821</td>\n",
       "      <td>0.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.681085</td>\n",
       "      <td>0.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669130</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.654443</td>\n",
       "      <td>0.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.647185</td>\n",
       "      <td>0.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.643139</td>\n",
       "      <td>0.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.640377</td>\n",
       "      <td>0.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.642200</td>\n",
       "      <td>0.641857</td>\n",
       "      <td>0.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.642200</td>\n",
       "      <td>0.642950</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.642200</td>\n",
       "      <td>0.641928</td>\n",
       "      <td>0.618000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\dev\\ai-nanodegree-project1\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-06-11 11:48:47.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mpeft trainer training complete\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Performing Parameter-Efficient Fine-Tuning\n",
    "## Creating a PEFT config\n",
    "# Create a PEFT config with appropriate hyperparameters for your chosen model.\n",
    "# lora_config = LoraConfig()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "\tlora_alpha=1,\n",
    "\tlora_dropout=0.1,\n",
    "\tr=1,\n",
    "\ttask_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "logger.info('peft config created')\n",
    "\n",
    "## Creating a PEFT model\n",
    "# Using the PEFT config and foundation model, create a PEFT model.\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "logger.info('lora model instantiated')\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "\tevaluation_strategy='epoch',\n",
    "\tload_best_model_at_end=True,\n",
    "\tnum_train_epochs=num_train_epochs,\n",
    "\toutput_dir=initial_save_name,\n",
    "\tsave_strategy='epoch',\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "\targs=peft_training_args,\n",
    "\tcompute_metrics=compute_metrics,\n",
    "\tmodel=model,\n",
    "\teval_dataset=sampled_ds[test_key],\n",
    "\ttrain_dataset=sampled_ds[train_key],\n",
    ")\n",
    "logger.info('peft trainer instantiated')\n",
    "\n",
    "## Training the model\n",
    "# Using the PEFT model and dataset, run a training loop with at least one epoch.\n",
    "peft_trainer.train()\n",
    "logger.info('peft trainer training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3558179-d76d-42ac-9426-97c13acf966d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-11 12:37:33.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mbeginning save...\u001b[0m\n",
      "\u001b[32m2024-06-11 12:37:34.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1msave complete.\u001b[0m\n",
      "\u001b[32m2024-06-11 12:37:34.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mloading saved model temp...\u001b[0m\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-06-11 12:37:36.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1m...model loaded\u001b[0m\n",
      "\u001b[32m2024-06-11 12:37:36.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mPeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BertForSequenceClassification(\n",
      "      (bert): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSdpaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Saving the trained model\n",
    "# Depending on your training loop configuration, your PEFT model may have already been saved. If not, use save_pretrained to save your progress.\n",
    "logger.info('beginning save...')\n",
    "model.save_pretrained(final_save_name)\n",
    "logger.info('save complete.')\n",
    "\n",
    "### Performing Inference with a PEFT Model\n",
    "## Loading the model\n",
    "# Using the appropriate PEFT model class, load your trained model.\n",
    "logger.info(f'loading saved model {final_save_name}...')\n",
    "final_model = AutoPeftModelForSequenceClassification.from_pretrained(final_save_name)\n",
    "logger.info('...model loaded')\n",
    "logger.info(final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b611176b-5426-4805-9e03-639066b01d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-11 12:37:41.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mfinal trainer instantiated\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 34:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-11 13:12:20.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mevaluation complete :)\u001b[0m\n",
      "\u001b[32m2024-06-11 13:12:20.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mThe final model was evaluated with the following metrics:\u001b[0m\n",
      "\u001b[32m2024-06-11 13:12:20.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1m{'eval_loss': 0.6403766870498657, 'eval_accuracy': 0.619, 'eval_runtime': 2079.2669, 'eval_samples_per_second': 0.481, 'eval_steps_per_second': 0.06}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Evaluating the model\n",
    "# Repeat the previous evaluation process, this time using the PEFT model. Compare the results to the results from the original foundation model.\n",
    "final_trainer = Trainer(\n",
    "\targs=peft_training_args,\n",
    "\tcompute_metrics=compute_metrics,\n",
    "\tmodel=final_model,\n",
    "\teval_dataset=sampled_ds[test_key],\n",
    "\ttrain_dataset=sampled_ds[train_key],\n",
    ")\n",
    "logger.info('final trainer instantiated')\n",
    "\n",
    "final_evaluation = final_trainer.evaluate()\n",
    "logger.info('evaluation complete :)')\n",
    "\n",
    "logger.info('The final model was evaluated with the following metrics:')\n",
    "logger.info(final_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd78fd4-472e-4d37-a718-ef1142cb959e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
